{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 15:02:13.183391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 15:02:14.920183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import faiss\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.],\n",
       "       [6.],\n",
       "       [7.],\n",
       "       [8.],\n",
       "       [9.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = np.arange(10).reshape(-1 ,1).astype('float32')\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(1)\n",
    "index.add(embeddings)\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(embeddings, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 3), (10, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape, I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 0\n",
      "[0.]: 0.0\n",
      "[1.]: 1.0\n",
      "[2.]: 4.0\n",
      "\n",
      "query: 1\n",
      "[1.]: 0.0\n",
      "[0.]: 1.0\n",
      "[2.]: 1.0\n",
      "\n",
      "query: 2\n",
      "[2.]: 0.0\n",
      "[1.]: 1.0\n",
      "[3.]: 1.0\n",
      "\n",
      "query: 3\n",
      "[3.]: 0.0\n",
      "[2.]: 1.0\n",
      "[4.]: 1.0\n",
      "\n",
      "query: 4\n",
      "[4.]: 0.0\n",
      "[3.]: 1.0\n",
      "[5.]: 1.0\n",
      "\n",
      "query: 5\n",
      "[5.]: 0.0\n",
      "[4.]: 1.0\n",
      "[6.]: 1.0\n",
      "\n",
      "query: 6\n",
      "[6.]: 0.0\n",
      "[5.]: 1.0\n",
      "[7.]: 1.0\n",
      "\n",
      "query: 7\n",
      "[7.]: 0.0\n",
      "[6.]: 1.0\n",
      "[8.]: 1.0\n",
      "\n",
      "query: 8\n",
      "[8.]: 0.0\n",
      "[7.]: 1.0\n",
      "[9.]: 1.0\n",
      "\n",
      "query: 9\n",
      "[9.]: 0.0\n",
      "[8.]: 1.0\n",
      "[7.]: 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, (d, i) in enumerate(zip(D, I)):\n",
    "    print(f'query: {num}')\n",
    "    for val, dist in zip(embeddings[i], d):\n",
    "        print(f'{val}: {dist}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(query_label, labels):\n",
    "    \n",
    "    # initialise mean hit ratio, mean reciprocal rank, and mean average precision\n",
    "    MHR, MRR, MAP = [], [], []\n",
    "    \n",
    "    # position, rank, and flag\n",
    "    pos, mrr_flag = 0, False\n",
    "    \n",
    "    # iterate over the neighbors\n",
    "    for rank, label in enumerate(labels):\n",
    "\n",
    "        # its a hit\n",
    "        if (query_label == label).all():\n",
    "            pos += 1\n",
    "            MHR.append(1)\n",
    "            MAP.append(pos/(rank+1))\n",
    "\n",
    "            # its the first hit\n",
    "            if not mrr_flag:\n",
    "                MRR.append(pos/(rank+1))\n",
    "                mrr_flag = True\n",
    "        \n",
    "        # its a miss\n",
    "        else:\n",
    "            MHR.append(0)\n",
    "            MAP.append(0)\n",
    "    \n",
    "    MRR = MRR[0] if len(MRR) else 0\n",
    "    \n",
    "    return sum(MAP)/len(MAP), sum(MHR)/len(MHR), MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243324\n"
     ]
    }
   ],
   "source": [
    "file_path = '/ssd_scratch/cvit/arihanth/physionet.org/files/generalized-image-embeddings-for-the-mimic-chest-x-ray-dataset-1.0/files'\n",
    "all_files = sorted(glob(f'{file_path}/**/*.tfrecord', recursive=True))\n",
    "print(len(all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 15:02:37.921431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2454 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5\n",
      "2023-09-19 15:02:37.922171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2568 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2023-09-19 15:02:37.922927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 2568 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:82:00.0, compute capability: 7.5\n",
      "2023-09-19 15:02:37.923486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 2568 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:83:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(d)   # build the index\n",
    "print(index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243324/243324 [01:23<00:00, 2905.49it/s]\n"
     ]
    }
   ],
   "source": [
    "all_emb = []\n",
    "\n",
    "for i, test in enumerate(tqdm(raw_dataset.take(-1), total=len(all_files))):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(test.numpy())\n",
    "    f_name = example.features.feature['image/id'].bytes_list.value[0].decode('utf-8').split('/')\n",
    "    assert f_name[6:9] == all_files[i].split('/')[8:11], f'{f_name[6:9]} != {all_files[i].split(\"/\")[8:11]}'\n",
    "    emb = np.array(example.features.feature['embedding'].float_list.value).astype(np.float32).reshape(1, -1)\n",
    "    all_emb.append(emb)\n",
    "    index.add(emb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243324\n"
     ]
    }
   ],
   "source": [
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "idx, k = 13245, 30\n",
    "for _, test in enumerate(tqdm(raw_dataset.skip(idx).take(1))):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(test.numpy())\n",
    "    f_name = example.features.feature['image/id'].bytes_list.value[0].decode('utf-8').split('/')\n",
    "    assert f_name[6:9] == all_files[idx].split('/')[8:11], f'{f_name[6:9]} != {all_files[idx].split(\"/\")[8:11]}'\n",
    "    emb = np.array(example.features.feature['embedding'].float_list.value).astype(np.float32).reshape(1, -1)\n",
    "    D, I = index.search(emb, k) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 30),\n",
       " (1, 30),\n",
       " array([  0.     , 105.08956, 221.67871, 228.2427 , 240.05519, 243.17981,\n",
       "        244.0354 , 249.12021, 250.36957, 250.50313, 252.62714, 254.26682,\n",
       "        255.83916, 256.03058, 263.09354, 264.16855, 267.72662, 268.4331 ,\n",
       "        268.6006 , 270.77734, 271.50287, 273.61182, 274.33374, 274.4439 ,\n",
       "        274.94873, 275.5473 , 275.92383, 275.99344, 276.56293, 276.8836 ],\n",
       "       dtype=float32),\n",
       " array([ 13245,  13246, 153752,  81398,   6727, 201689, 123850,  30607,\n",
       "        108081,  45245,  63107,  66697, 123681, 219862,   1296,  95856,\n",
       "         51507, 190085, 158009, 132451,  27778, 137348, 118796,  71700,\n",
       "         59210,  44520, 110218, 204313,  98682, 171858]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape, I.shape, D[0], I[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.mimic_cxr_emb import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = CustomDataset('/ssd_scratch/cvit/arihanth/physionet.org/files/generalized-image-embeddings-for-the-mimic-chest-x-ray-dataset-1.0/files', 'all', -1)\n",
    "my_loader  = torch.utils.data.DataLoader(my_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1257, -1.8030,  1.2843,  ..., -0.7077,  1.0860,  0.0256]]) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "sample_emb, sample_label = next(iter(my_loader))\n",
    "print(sample_emb, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.     , 105.08956, 221.67871, 228.2427 , 240.05519, 243.17981,\n",
       "        244.0354 , 249.12021, 250.36957, 250.50313, 252.62714, 254.26682,\n",
       "        255.83916, 256.03058, 263.09354, 264.16855, 267.72662, 268.4331 ,\n",
       "        268.6006 , 270.77734, 271.50287, 273.61182, 274.33374, 274.4439 ,\n",
       "        274.94873, 275.5473 , 275.92383, 275.99344, 276.56293, 276.8836 ],\n",
       "       dtype=float32),\n",
       " array([ 13245,  13246, 153752,  81398,   6727, 201689, 123850,  30607,\n",
       "        108081,  45245,  63107,  66697, 123681, 219862,   1296,  95856,\n",
       "         51507, 190085, 158009, 132451,  27778, 137348, 118796,  71700,\n",
       "         59210,  44520, 110218, 204313,  98682, 171858]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[0], I[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " [tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, query_label = my_dataset.__getitem__(I[0][0])\n",
    "labels = [my_dataset.__getitem__(i)[1] for i in I[0][1:]]\n",
    "\n",
    "query_label, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7860952051160411, 0.896551724137931, 1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(query_label, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [21:44<00:00,  2.74s/it, mAP=0.221, mHR=0.258, mRR=0.812]\n"
     ]
    }
   ],
   "source": [
    "mAP, mHR, mRR = [], [], []\n",
    "\n",
    "with tqdm(my_loader) as pbar:\n",
    "    for emb, query_labels in pbar:\n",
    "        D, I = index.search(emb, 5)\n",
    "\n",
    "        labels = [[my_dataset.__getitem__(i)[1] for i in I[j][1:]] for j in range(I.shape[0])]\n",
    "\n",
    "        for query_label, target_label in zip(query_labels, labels):\n",
    "            MAP, MHR, MRR = get_metrics(query_label, target_label)\n",
    "            mAP.append(MAP)\n",
    "            mHR.append(MHR)\n",
    "            if MRR:\n",
    "                mRR.append(MRR)\n",
    "        \n",
    "        pbar.set_postfix({'mAP': sum(mAP)/len(mAP), 'mHR': sum(mHR)/len(mHR), 'mRR': sum(mRR)/len(mRR)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not batched, takes too long\n",
    "\n",
    "mAP, mHR, mRR = [], [], []\n",
    "\n",
    "with tqdm(all_emb) as pbar:\n",
    "    for emb in pbar:\n",
    "        D, I = index.search(emb, 5)\n",
    "\n",
    "        _, query_label = my_dataset.__getitem__(I[0][0])\n",
    "        labels = [my_dataset.__getitem__(i)[1] for i in I[0][1:]]\n",
    "\n",
    "        MAP, MHR, MRR = get_metrics(query_label, labels)\n",
    "        mAP.append(MAP)\n",
    "        mHR.append(MHR)\n",
    "        if MRR:\n",
    "            mRR.append(MRR)\n",
    "        \n",
    "        pbar.set_postfix({'mAP': sum(mAP)/len(mAP), 'mHR': sum(mHR)/len(mHR), 'mRR': sum(mRR)/len(mRR)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
